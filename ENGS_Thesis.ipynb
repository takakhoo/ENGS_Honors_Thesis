{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first sell below are installing and testing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.2.2\n",
      "torchaudio version: 2.2.2\n",
      "Librosa version: 0.10.2.post1\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.2.3\n",
      "Matplotlib version: 3.9.2\n",
      "SciPy version: 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "import torchaudio\n",
    "print(\"torchaudio version:\", torchaudio.__version__)\n",
    "\n",
    "import librosa\n",
    "print(\"Librosa version:\", librosa.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "\n",
    "import scipy\n",
    "print(\"SciPy version:\", scipy.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton Code\n",
    "\n",
    "Data Loading & Preprocessing:\n",
    "The function load_audio_compute_spectrogram uses Librosa to load an audio file and compute its STFT. It then extracts the magnitude (i.e., the absolute value) and normalizes it.\n",
    "\n",
    "Model Definition:\n",
    "The AudioAutoencoder is a simple convolutional autoencoder. The encoder compresses the input spectrogram to a lower-dimensional representation; the decoder attempts to reconstruct the original spectrogram from this representation.\n",
    "\n",
    "Training Loop:\n",
    "The train_model function provides a basic loop where each batch (here, for simplicity, a list of identical tensors) is passed through the model. The loss (MSE) is computed between the input and the output, and the model parameters are updated using the Adam optimizer.\n",
    "\n",
    "Inference, Visualization, & Reconstruction:\n",
    "After training, the model is used to reconstruct the input spectrogram.\n",
    "Both the original and reconstructed spectrograms are displayed side-by-side using Matplotlib and Librosa’s display functions.\n",
    "Finally, the Griffin–Lim algorithm is used to reconstruct an audio waveform from the (reconstructed) magnitude spectrogram, and the result is saved to disk with SoundFile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/hqntk89s26gdy0qr2rbl_gm80000gn/T/ipykernel_55324/1614930947.py:26: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(file_path, sr=self.sr)\n",
      "/Users/takakhoo/miniconda3/envs/thesis_env/lib/python3.9/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.230520\n",
      "Epoch 2/10, Loss: 0.224115\n",
      "Epoch 3/10, Loss: 0.216698\n",
      "Epoch 4/10, Loss: 0.211377\n",
      "Epoch 5/10, Loss: 0.203321\n",
      "Epoch 6/10, Loss: 0.197033\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Custom PyTorch dataset, load files from a specified directory, normalize into mag spectrogram\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, directory, sr=22050, n_fft=1024, hop_length=512, transform=None):\n",
    "        self.file_list = glob.glob(os.path.join(directory, '*.wav'))\n",
    "        self.sr = sr\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        audio, sr = librosa.load(file_path, sr=self.sr)\n",
    "        stft = librosa.stft(audio, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "        magnitude = np.abs(stft)\n",
    "        magnitude = magnitude/np.max(magnitude)\n",
    "        spectrogram_tensor = torch.tensor(magnitude, dtype=torch.float32).unsqueeze(0)\n",
    "        if self.transform:\n",
    "            spectrogram_tensor = self.transform(spectrogram_tensor)\n",
    "        return spectrogram_tensor, file_path\n",
    "\n",
    "# This hack fixes the multiprocessing pickling error by ensuring that the AudioDataset\n",
    "# class is available as an attribute of the __main__ module.\n",
    "import __main__\n",
    "__main__.AudioDataset = AudioDataset\n",
    "\n",
    "# Custom collate function to pad variable-length spectrogram tensors\n",
    "def custom_collate_fn(batch):\n",
    "    # batch is a list of tuples: (spectrogram_tensor, file_path)\n",
    "    tensors, paths = zip(*batch)\n",
    "    # Each tensor has shape [1, 513, time]\n",
    "    max_time = max(t.shape[2] for t in tensors)\n",
    "    padded_tensors = []\n",
    "    for t in tensors:\n",
    "        time_dim = t.shape[2]\n",
    "        if time_dim < max_time:\n",
    "            # Pad along the time dimension (last dimension); pad format: (pad_left, pad_right)\n",
    "            import torch.nn.functional as F\n",
    "            t = F.pad(t, (0, max_time - time_dim), mode='constant', value=0)\n",
    "        padded_tensors.append(t)\n",
    "    batch_tensor = torch.stack(padded_tensors, dim=0)\n",
    "    return batch_tensor, list(paths)\n",
    "\n",
    "#Simple NN Model\n",
    "class AudioAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioAutoEncoder, self).__init__()\n",
    "        \n",
    "        #Two convolutional layers reduce spatial dimensions\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=1, stride=2, padding=1), #Output (batch, 16, H/2, W/2)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=1, stride=2, padding=1), #Output (batch, 32, H/4, W/4)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #Two transposed Convolution restore OG dimensions\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid() #Quick check on outputs between 0 and 1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded=self.encoder(x)\n",
    "        decoded=self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "#Skeleton for training loop\n",
    "def train_model(model, data, epochs=10, lr=1e-3, checkpoint_dir = 'checkpoints'):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss() #Mean Squared Error loss for reconstruction\n",
    "    writer = SummaryWriter(log_dir='runs/audio_autoencoder')\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        model.train()\n",
    "        for i, (batch, file_path) in enumerate(data):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch)\n",
    "            if output.shape != batch.shape:\n",
    "                target_h, target_w = batch.shape[2], batch.shape[3] #Cropping output if there's a mismatch\n",
    "                output = output[:, :, :target_h, :target_w]\n",
    "            loss = criterion(output, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            writer.add_scalar('Batch Loss', loss.item(), epoch * len(data)+i)\n",
    "        avg_loss = epoch_loss / len(data)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(data):.6f}\")\n",
    "        writer.add_scalar('Epoch Loss', avg_loss, epoch)\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "#Reconstruct and save audio outputs\n",
    "def evaluate_model(model, dataloader, output_dir='outputs', n_iter=32, hop_length=512, win_length=1024):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, file_paths) in enumerate(dataloader):\n",
    "            batch = batch.to(device)\n",
    "            reconstructed = model(batch)\n",
    "            # Crop if necessary\n",
    "            if reconstructed.shape != batch.shape:\n",
    "                target_h, target_w = batch.shape[2], batch.shape[3]\n",
    "                reconstructed = reconstructed[:, :, :target_h, :target_w]\n",
    "            # Convert to NumPy array and remove extra dimensions\n",
    "            reconstructed_np = reconstructed.cpu().numpy()  # shape: (B, C, H, W)\n",
    "            B = reconstructed_np.shape[0]\n",
    "            for j in range(B):\n",
    "                rec_sample = np.squeeze(reconstructed_np[j])  # shape: (H, W)\n",
    "                # Use Griffin–Lim algorithm to convert magnitude spectrogram to audio\n",
    "                reconstructed_audio = librosa.griffinlim(rec_sample, n_iter=n_iter, hop_length=hop_length, win_length=win_length)\n",
    "                base_filename = os.path.basename(file_paths[j])\n",
    "                output_path = os.path.join(output_dir, 'reconstructed_' + base_filename)\n",
    "                sf.write(output_path, reconstructed_audio, 22050)\n",
    "                print(f\"Reconstructed audio saved to {output_path}\")\n",
    "\n",
    "#Main Routine to load data, train, and inference\n",
    "if __name__==\"__main__\":\n",
    "     # Directory where your raw multitrack (or single track) audio files are stored\n",
    "    audio_dir = 'data'  # <-- Ensure you have a 'data' directory with .wav files\n",
    "    \n",
    "    # Create dataset and DataLoader for efficient streaming\n",
    "    dataset = AudioDataset(audio_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "    \n",
    "    # Initialize the autoencoder model\n",
    "    model = AudioAutoEncoder()\n",
    "    \n",
    "    # Train the model on the dataset (this will load files on the fly)\n",
    "    trained_model = train_model(model, dataloader, epochs=10, lr=1e-3)\n",
    "    \n",
    "    # Evaluate the trained model and reconstruct audio for all samples in the dataset\n",
    "    evaluate_model(trained_model, dataloader)\n",
    "    \n",
    "    # Optional: Visualize a sample reconstruction for comparison\n",
    "    sample_tensor, sample_path = dataset[0]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_reconstructed = model(sample_tensor.unsqueeze(0))\n",
    "    sample_reconstructed_np = sample_reconstructed.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Load the original spectrogram for comparison\n",
    "    _, original_spectrogram, sr = load_audio_compute_spectrogram(sample_path)\n",
    "    #Showing the before and after spectrograms\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    librosa.display.specshow(original_spectrogram, sr=sr, hop_length=512, x_axis='time', y_axis='log')\n",
    "    plt.title('Original Spectrogram')\n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    librosa.display.specshow(sample_reconstructed_np, sr=sr, hop_length=512, x_axis='time', y_axis='log')\n",
    "    plt.title('Reconstructed Spectrogram')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    #Griffin Lim method reconstructs audio from spectrogram \n",
    "    reconstructed_audio = librosa.griffinlim(sample_reconstructed_np, n_iter=32, hop_length=512, win_length=1024)\n",
    "\n",
    "    output_path = 'reconstructed_output.wav'\n",
    "    sf.write(output_path, reconstructed_audio, sr)\n",
    "    print(f\"Reconstructed audio saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps:\n",
    "\n",
    "Expand the Data Pipeline: Replace the dummy dataset with a proper DataLoader that reads and preprocesses your collection of raw multitracks.\n",
    "\n",
    "Model Improvements: Experiment with deeper architectures or incorporate additional techniques (e.g., GANs or perceptual loss functions) as you refine your approach.\n",
    "\n",
    "Integration with Logic Pro: Once you have reliable outputs, you can export high-quality WAV files to import into Logic Pro for further manual evaluation and mixing.\n",
    "\n",
    "Documentation & Evaluation: Record your code modifications, training experiments, and evaluation metrics as you move toward your thesis paper and eventual IEEE submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
